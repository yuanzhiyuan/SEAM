{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demostrate how SEAM package is designed. The functions within the notebook are arranged to different modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SEAM.datasets\n",
    "# raw dataset/processed dataset\n",
    "import SEAM.tl.Cut\n",
    "# load segmentation result, and get anndata, prepare obs/obsm, e.g. spatial...\n",
    "import SEAM.tl.ID\n",
    "# run ID and add ID representation to adata\n",
    "import SEAM.tl.Umap\n",
    "# run UMAP using SIMSID/mean\n",
    "import SEAM.tl.Cluster\n",
    "# run clustering algorithms, updata obs cluster of anndata\n",
    "import SEAM.tl.Diff\n",
    "# run differential methods, either SIMS-Diff or single cell methods, update anndata\n",
    "import SEAM.tl.View\n",
    "# run SIMS-View, update anndata, user can select color space and DM methods\n",
    "import SEAM.tl.ME\n",
    "# input list of points to form polylines, update anndata as boxplot/lineplot associated variables.\n",
    "\n",
    "\n",
    "import SEAM.pl.IMS\n",
    "# raw data ploting\n",
    "import SEAM.pl.Cut\n",
    "# ploting cut result, dot, real, or overlay of real/134\n",
    "import SEAM.pl.ID\n",
    "# ploting umap/PCA/Tsne of ID\n",
    "import SEAM.pl.Cluster\n",
    "# ploting spatial single cell map dot/real\n",
    "import SEAM.pl.Diff\n",
    "# plot heatmap given a set of genes\n",
    "import SEAM.pl.View\n",
    "# plot SIMS-View\n",
    "import SEAM.pl.ME\n",
    "# plot polyline overlay with cut/clustering, plot boxplot/lineplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import palettable\n",
    "sc.set_figure_params(dpi=500, color_map='viridis',dpi_save=500,transparent=True)\n",
    "\n",
    "sc.settings.verbosity = 2\n",
    "heatmap_cmp = palettable.cmocean.diverging.Balance_20.mpl_colormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "\n",
    "DATA_PATH_IMS_PROCESSED = '/home/yzy/SEAM/data/process/'\n",
    "DATA_PATH_DUMP = '/home/yzy/SEAM/data/dump/'\n",
    "def get_train_data(data_mat_filename,mode,norm,batch_num_list=[1]):\n",
    "\n",
    "\n",
    "\n",
    "    original_data = {}\n",
    "    cell_related_data = {}\n",
    "    data_mat = sio.loadmat(data_mat_filename)\n",
    "    data_mat=data_mat['data_mat']\n",
    "\n",
    "\n",
    "\n",
    "    num_features = data_mat.shape[1]-3\n",
    "    batch_dict = {}\n",
    "\n",
    "    label_dict = {}\n",
    "    cell_dict = {}\n",
    "    pos_dict = {}\n",
    "\n",
    "    for i in range(1):\n",
    "\n",
    "            cur_data = data_mat[data_mat[:,0]==i+1,3:num_features+3]\n",
    "\n",
    "\n",
    "            batch_dict[i+1] = cur_data\n",
    "\n",
    "            cell_dict[i+1] = data_mat[data_mat[:,0]==i+1,1]\n",
    "            cur_batch_idx = data_mat[data_mat[:,0]==i+1,2]\n",
    "            label_dict[i+1] = np.ones(shape=cur_batch_idx.shape)\n",
    "\n",
    "\n",
    "            pos_dict[i+1] = cur_batch_idx\n",
    "    original_data['batch_dict'] = batch_dict\n",
    "    original_data['cell_dict'] = cell_dict\n",
    "    original_data['label_dict'] = label_dict\n",
    "    original_data['pos_dict'] = pos_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    top_n_var = 250\n",
    "    train_x_all = None\n",
    "    cell_idx_all = None\n",
    "    cell_type_all = None\n",
    "    cell_pos_all = None\n",
    "    batch_idx_all = None\n",
    "    num_cells_all = 0\n",
    "\n",
    "\n",
    "    for batch_num in batch_num_list:\n",
    "        train_x = batch_dict[batch_num]\n",
    "        # train_x = eval('batch_dict_{norm_type}[batch_num]'.format(norm_type=norm_type))\n",
    "        # train_x = batch_dict[batch_num]\n",
    "        cell_idx = cell_dict[batch_num]\n",
    "        cell_type = label_dict[batch_num]\n",
    "        cell_pos = pos_dict[batch_num]\n",
    "        # batch_FE = FE_dict[batch_num]\n",
    "        cell_related_ind = (cell_idx!=0)\n",
    "\n",
    "        num_cells = int(np.max(cell_idx))\n",
    "        # num_cells = 2\n",
    "        train_x = train_x[cell_related_ind,:]\n",
    "\n",
    "        cell_idx = cell_idx[cell_related_ind]\n",
    "        cell_type = cell_type[cell_related_ind]\n",
    "        # cell_type = np.ones(shape=cell_idx.shape)\n",
    "        cell_pos = cell_pos[cell_related_ind]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        var_li = []\n",
    "        normed_var_li = []\n",
    "        for i in range(train_x.shape[1]):\n",
    "            cur_col = train_x[:,i]\n",
    "        #     cur_col= cur_row/np.sum(cur_col)\n",
    "        #     cur_entropy = entropy(cur_col)\n",
    "            cur_var = np.var(cur_col)\n",
    "            cur_normed_var = cur_var/np.mean(cur_col)\n",
    "        #     entropy_li.append(cur_entropy)\n",
    "            var_li.append(cur_var)\n",
    "            normed_var_li.append(cur_normed_var)\n",
    "        # entropy_li = np.array(entropy_li)\n",
    "        var_li = np.array(var_li)\n",
    "        normed_var_li = np.array(normed_var_li)\n",
    "        sort_ind = np.flip(np.argsort(normed_var_li),axis=0)\n",
    "        sort_val = np.flip(np.sort(normed_var_li),axis=0)\n",
    "        \n",
    "        if train_x_all is None:\n",
    "            train_x_all = train_x\n",
    "        else:\n",
    "            train_x_all = np.vstack([train_x_all,train_x])\n",
    "        if cell_idx_all is None:\n",
    "            cell_idx_all = cell_idx\n",
    "        else:\n",
    "            cell_idx_all = np.hstack([cell_idx_all,cell_idx+np.max(cell_idx_all)])\n",
    "        if cell_type_all is None:\n",
    "            cell_type_all = cell_type\n",
    "        else:\n",
    "            cell_type_all = np.hstack([cell_type_all,cell_type])\n",
    "        if cell_pos_all is None:\n",
    "            cell_pos_all = cell_pos\n",
    "        else:\n",
    "            cell_pos_all = np.hstack([cell_pos_all,cell_pos])\n",
    "        if batch_idx_all is None:\n",
    "            batch_idx_all = batch_num*np.ones(shape=(cell_idx.shape))\n",
    "        else:\n",
    "            batch_idx_all = np.hstack([batch_idx_all,batch_num*np.ones(shape=(cell_idx.shape))])\n",
    "\n",
    "\n",
    "    train_x = train_x_all\n",
    "    cell_idx = cell_idx_all\n",
    "    cell_type = cell_type_all\n",
    "    cell_pos = cell_pos_all\n",
    "    batch_idx = batch_idx_all\n",
    "    num_cells = np.max(cell_idx)\n",
    "    if mode=='none':\n",
    "        train_x = train_x\n",
    "    elif mode=='median':\n",
    "        train_x = train_x/np.percentile(train_x,50,axis=1,keepdims=True)\n",
    "        train_x = np.log(train_x+1)\n",
    "    elif mode=='total':\n",
    "        train_x = train_x/np.sum(train_x,axis=1,keepdims=True)\n",
    "        train_x = np.log(train_x+1)\n",
    "\n",
    "    if norm=='standard':\n",
    "        train_x = StandardScaler().fit_transform(train_x)\n",
    "    elif norm=='l1':\n",
    "        train_x = Normalizer(norm='l1').fit_transform(train_x)\n",
    "    elif norm=='l2':\n",
    "        train_x = Normalizer(norm='l2').fit_transform(train_x)\n",
    "    elif norm=='none':\n",
    "        train_x = train_x\n",
    "\n",
    "    cell_related_data['train_x'] = train_x\n",
    "    cell_related_data['cell_idx'] = cell_idx\n",
    "    cell_related_data['cell_type'] = cell_type\n",
    "    cell_related_data['cell_pos'] = cell_pos\n",
    "    cell_related_data['batch_idx'] =batch_idx\n",
    "    cell_related_data['num_cells'] = num_cells\n",
    "    return original_data,cell_related_data\n",
    "\n",
    "\n",
    "\n",
    "def load_raw_SIMS(data):\n",
    "    data_mat_filename_temp = DATA_PATH_IMS_PROCESSED+'{0}/cut/rst/datamat.mat'\n",
    "    matter_list_filename_temp=DATA_PATH_IMS_PROCESSED+'{0}/preprocess/matters_candidate.pkl'\n",
    "\n",
    "#     data = 'P6_neg1_low0_None_auto'\n",
    "    test_sample_temp=DATA_PATH_IMS_PROCESSED+'{0}/preprocess/test_samples.mat'\n",
    "\n",
    "\n",
    "\n",
    "    matter_list_filename = matter_list_filename_temp.format(data)\n",
    "    data_mat_filename = data_mat_filename_temp.format(data)\n",
    "    test_sample_filename = test_sample_temp.format(data)\n",
    "    test_sample_all = sio.loadmat(test_sample_filename)['test_samples']\n",
    "    mode='none'\n",
    "    norm='none'\n",
    "    [original_data,cell_related_data]=get_train_data(data_mat_filename,mode,norm,batch_num_list=[1])\n",
    "    train_x=cell_related_data['train_x']\n",
    "    cell_idx=cell_related_data['cell_idx']\n",
    "    cell_pos=cell_related_data['cell_pos']\n",
    "    num_cells = np.max(cell_idx)\n",
    "    matter_list = pickle.load(open(matter_list_filename,'rb'))\n",
    "    matter_list = np.array(matter_list)\n",
    "\n",
    "    return train_x,cell_idx,cell_pos,matter_list,num_cells,test_sample_all\n",
    "\n",
    "\n",
    "def get_mean_representation(train_x,cell_idx,num_cells):\n",
    "    train_x_tmp = train_x.copy()\n",
    "    train_x_median = (train_x_tmp+1)/(np.percentile(train_x_tmp,50,axis=1,keepdims=True)+1)\n",
    "    train_x_total = train_x/np.sum(train_x,axis=1,keepdims=True)\n",
    "    train_x_median = np.log(train_x_median+1)\n",
    "    train_x_total = np.log(train_x_total+1)\n",
    "    # train_x_A = (train_x+1)/(train_x[:,matter_list==134.06]+1)\n",
    "    sum_profile_list_median = []\n",
    "    sum_profile_list_total = []\n",
    "\n",
    "    max_profile_list_median = []\n",
    "    max_profile_list = []\n",
    "    mean_profile_list_median = []\n",
    "    max_profile_list_total = []\n",
    "    mean_profile_list_total = []\n",
    "    mean_profile_list=[]\n",
    "    # mean_profile_list_A=[]\n",
    "    # max_profile_list_A = []\n",
    "    for i in range(num_cells):\n",
    "            mean_profile_list_median.append(np.mean(train_x_median[cell_idx==i+1,:],axis=0))\n",
    "            max_profile_list_median.append(np.max(train_x_median[cell_idx==i+1,:],axis=0))\n",
    "            max_profile_list.append(np.max(train_x[cell_idx==i+1,:],axis=0))\n",
    "    #         mean_profile_list_A.append(np.mean(train_x_A[cell_idx==i+1,:],axis=0))\n",
    "    #         max_profile_list_A.append(np.max(train_x_A[cell_idx==i+1,:],axis=0))\n",
    "\n",
    "            sum_profile_list_median.append(np.sum(train_x_median[cell_idx==i+1,:],axis=0))\n",
    "            mean_profile_list.append(np.mean(train_x[cell_idx==i+1,:],axis=0))\n",
    "            mean_profile_list_total.append(np.mean(train_x_total[cell_idx==i+1,:],axis=0))\n",
    "            max_profile_list_total.append(np.max(train_x_total[cell_idx==i+1,:],axis=0))\n",
    "            sum_profile_list_total.append(np.sum(train_x_total[cell_idx==i+1,:],axis=0))\n",
    "\n",
    "    mean_profile_list_median = np.array(mean_profile_list_median)\n",
    "    max_profile_list_median = np.array(max_profile_list_median)\n",
    "    mean_profile_list_total = np.array(mean_profile_list_total)\n",
    "    max_profile_list_total = np.array(max_profile_list_total)\n",
    "    sum_profile_list_median = np.array(sum_profile_list_median)\n",
    "    sum_profile_list_total = np.array(sum_profile_list_total)\n",
    "    mean_profile_list = np.array(mean_profile_list)\n",
    "    max_profile_list = np.array(max_profile_list)\n",
    "    # mean_profile_list_A = np.array(mean_profile_list_A)\n",
    "    # max_profile_list_A = np.array(max_profile_list_A)\n",
    "    return mean_profile_list_median\n",
    "\n",
    "\n",
    "def load_dataset_raw(data):\n",
    "    train_x,cell_idx,cell_pos,matter_list,num_cells,test_sample_all = load_raw_SIMS(data)\n",
    "    mean_profile_list_median = get_mean_representation(train_x,cell_idx,num_cells)\n",
    "    \n",
    "    in_X = mean_profile_list_median\n",
    "    # g = map(str,range(in_X.shape[1]))\n",
    "    g = map(str,matter_list)\n",
    "    Genes = []\n",
    "    None_idx = 0\n",
    "    Genes = g\n",
    "    # obs_name must be str\n",
    "    obs_name = list(map(str,range(in_X.shape[0])))\n",
    "    obs = pd.DataFrame(index=obs_name)\n",
    "\n",
    "    # var_name must be str\n",
    "    var = pd.DataFrame(index=Genes)\n",
    "\n",
    "    #     var['Genes'] = Genes\n",
    "    a = ad.AnnData(in_X,  obs=obs,var=var, dtype='float32')\n",
    "    a.uns['cell_idx'] = cell_idx\n",
    "    a.uns['cell_pos'] = cell_pos\n",
    "    a.uns['IMS'] = test_sample_all\n",
    "    a.uns['train_x'] = train_x\n",
    "#     a.uns['test_sample_all'] = \n",
    "#     a.uns['rep_list'] = rep_list\n",
    "    return a\n",
    "\n",
    "\n",
    "def load_dataset_processed(data):\n",
    "    data_dump_path = DATA_PATH_DUMP+'{0}/data.h5ad'.format(data)\n",
    "    \n",
    "    a = ad.read_h5ad(data_dump_path)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_dataset_raw('P6_neg1_low0_None_auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.uns['train_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_dataset_processed('P1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_matters(key_matters,matter_list):\n",
    "    key_matters = np.array(key_matters).astype('float')\n",
    "    matter_list = np.array(matter_list).astype('float')\n",
    "    adjested_key_matters = []\n",
    "    for k in key_matters:\n",
    "        m_diff = np.abs(matter_list-k)\n",
    "        min_diff_idx = np.argmin(m_diff)\n",
    "        min_diff_m = matter_list[min_diff_idx]\n",
    "        adjested_key_matters.append(min_diff_m)\n",
    "    adjested_key_matters = np.array(adjested_key_matters)\n",
    "    return adjested_key_matters\n",
    "\n",
    "def add_subcls(a_cls_sub,ec_cls,to_replace_cls):\n",
    "    a_cls_sub_int = a_cls_sub.astype('int')\n",
    "    a_cls_sub_int[a_cls_sub_int==int(to_replace_cls)] = -1\n",
    "    a_cls_sub_int[a_cls_sub_int>int(to_replace_cls)]-=1\n",
    "    a_cls_sub_int_max = a_cls_sub_int.max()\n",
    "    \n",
    "    ec_cls_int = ec_cls.astype('int')\n",
    "    ec_cls_int -= ec_cls_int.min()\n",
    "    ec_cls_int+=(a_cls_sub_int_max+1)\n",
    "    a_cls_sub_int[a_cls_sub_int==-1] = ec_cls_int\n",
    "    return a_cls_sub_int.astype('str')\n",
    "\n",
    "def ind2ij(ind,size,axis):\n",
    "    i,j=divmod(ind-1,size)\n",
    "    i+=1\n",
    "    j+=1\n",
    "    return np.array([i,j])[axis]\n",
    "\n",
    "def get_labeling(label,cell_idx,cell_pos):\n",
    "#     y是cell-rela的细胞对应的标签\n",
    "#     print('pred_y',np.unique(label))\n",
    "    labeling = np.zeros(shape=(65536,1))\n",
    "    b = cell_idx.copy()\n",
    "    num_cells = label.shape[0]\n",
    "    for i in range(num_cells):\n",
    "        b[b==i+1] = label[i] + 1\n",
    "#     print(cell_pos)\n",
    "#     print('b',np.unique(b))\n",
    "#     cell_pos = cell_pos.astype('int')\n",
    "    labeling[cell_pos.astype('int')-1,0] = b\n",
    "\n",
    "    return labeling\n",
    "\n",
    "def plot_label_image(a,pred_y,cmp,save=None,mask=None,figsize=(5,5),anno=False,ifshow=True):\n",
    "    \n",
    "    cell_idx = a.uns['cell_idx']\n",
    "    cell_pos = a.uns['cell_pos']\n",
    "    \n",
    "    to_labeling_pred_y = np.array(pred_y.astype('int'))\n",
    "    to_labeling_pred_y_min = to_labeling_pred_y.min()\n",
    "    # to_labeling_pred_y[coc[448,:]>0]=3\n",
    "    # to_labeling_pred_y = resultsLWEA[:,2]\n",
    "    # to_labeling_pred_y = label_list_FF[2]\n",
    "    # to_labeling_pred_y = label_list[3]\n",
    "    # to_labeling_pred_y[mark_list]=2\n",
    "#     cluster_cmp = sns.hls_palette(np.unique(to_labeling_pred_y).shape[0])\n",
    "    unique_cls = np.unique(pred_y).shape[0]\n",
    "#     unique_cls_mask = [unique_cls[m] for m in mask]\n",
    "    cluster_cmp = cmp.copy()\n",
    "    \n",
    "    if mask is not None:\n",
    "        for to_mask in range(unique_cls):\n",
    "            if to_mask in mask:\n",
    "                continue\n",
    "            cluster_cmp[to_mask]='k'\n",
    "    labeling_plot_cmp = ['k']\n",
    "    labeling_plot_cmp.extend(cluster_cmp)\n",
    "    labeling = get_labeling(to_labeling_pred_y-np.min(to_labeling_pred_y),cell_idx,cell_pos)\n",
    "    # labeling[labeling==5]=0\n",
    "    img1 = labeling.reshape((256,256))\n",
    "    plt.figure(figsize=figsize)\n",
    "    # plt.imshow(img1)\n",
    "    ticks=np.arange(np.min(img1)+1,np.max(img1)+1)\n",
    "    boundaries = np.arange(np.min(img1)+0.5,np.max(img1)+1.5)\n",
    "#     with sns.plotting_context(font_scale=font_scale):\n",
    "    sns.heatmap(img1,cmap=labeling_plot_cmp,linewidths=0,linecolor='k',square=True,cbar_kws={\"ticks\":ticks, \"boundaries\":boundaries,'fraction':0.046,'pad':0.04})\n",
    "    # sns.heatmap(img1,cmap=labeling_plot_cmp,square=True,ad':0.04})\n",
    "#     plt.legend(fontsize=font_size)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if save is not None:\n",
    "        plt.savefig(save,transparent=False,format='png',bbox_inches='tight')\n",
    "\n",
    "    if anno:\n",
    "        num_cells = pred_y.shape[0]\n",
    "        for i in range(num_cells):\n",
    "            cur_idx = i + 1\n",
    "            cur_ind = cell_pos[cell_idx==cur_idx][0]\n",
    "        #     print(ind2ij(cur_ind,256,0))\n",
    "        #     print(ind2ij(cur_ind,256,1))\n",
    "            if to_labeling_pred_y[i]-to_labeling_pred_y_min in mask:\n",
    "                plt.annotate(str(cur_idx-1),(ind2ij(cur_ind,256,1),ind2ij(cur_ind,256,0)),color='red')\n",
    "\n",
    "    if ifshow:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut(a):\n",
    "    num_cells = a.shape[0]\n",
    "    cell_idx = a.uns['cell_idx']\n",
    "    cell_pos = a.uns['cell_pos']\n",
    "    cell_pos_list = []\n",
    "\n",
    "\n",
    "    for i in range(num_cells):\n",
    "        cur_idx = i + 1\n",
    "        cur_ind = cell_pos[cell_idx==cur_idx][0]\n",
    "        cur_ind_list = cell_pos[cell_idx==cur_idx]\n",
    "        cur_x_list = []\n",
    "        cur_y_list = []\n",
    "        for j in cur_ind_list:\n",
    "            cur_x = ind2ij(cur_ind,256,1)\n",
    "            cur_y = ind2ij(cur_ind,256,0)\n",
    "            cur_x_list.append(cur_x)\n",
    "            cur_y_list.append(cur_y)\n",
    "        cur_x_mean = np.mean(cur_x_list)\n",
    "        cur_y_mean = np.mean(cur_y_list)\n",
    "\n",
    "        cell_pos_list.append(np.array([cur_x_mean,cur_y_mean]))\n",
    "\n",
    "\n",
    "    cell_pos_mat = np.array(cell_pos_list)\n",
    "    print('setting obsm: spatial')\n",
    "    a.obsm['spatial'] = cell_pos_mat\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Cut(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "sc.pl.embedding(a,basis='spatial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Umap(a,rep='ID'):\n",
    "    if rep=='ID':\n",
    "        sc.pp.neighbors(a,use_rep='ID',metric='cosine',n_neighbors=15)\n",
    "    else:\n",
    "        sc.pp.neighbors(a,n_neighbors=15)\n",
    "    sc.tl.umap(a)\n",
    "    print('Sucessfully run Umap!')\n",
    "    return a\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.constraints import *\n",
    "from keras.regularizers import *\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "\n",
    "\n",
    "\n",
    "def get_distil_rep(train_x,cell_idx,num_cells,t_list,epochs=50,verbose=False,activa = 'relu',dp_rate = 0.5,low_dim = 128,l2_penalty = 0,l1_penalty = 1e-5,use_bias = False,netwidths=[512,256,128],error_threshold=2):\n",
    "\n",
    "    SIMS_input = Input(shape=(train_x.shape[1],))\n",
    "    target_input = Input(shape=(1,))\n",
    "    kernel_init_func = glorot_normal()\n",
    "\n",
    "    d1 = Dense(netwidths[0],activation=activa,kernel_initializer=kernel_init_func,kernel_regularizer=l2(l2_penalty),use_bias=use_bias)(SIMS_input)\n",
    "    # d1 = Dense(1024,activation=activa,kernel_initializer='random_uniform')(SIMS_input)\n",
    "    # d1 = Dense(1024,activation=activa,kernel_initializer='random_uniform')(d1)\n",
    "\n",
    "\n",
    "    d2 = Dense(netwidths[1],activation=activa,kernel_initializer=kernel_init_func,kernel_regularizer=l2(l2_penalty),use_bias=use_bias)(d1)\n",
    "    # d2 = Dense(512,activation=activa,kernel_initializer='random_uniform')(d1)\n",
    "    # d2 = Dense(512,activation=activa,kernel_initializer='random_uniform')(d2)\n",
    "    d2 = Dense(netwidths[2],activation='linear',kernel_initializer=kernel_init_func,use_bias=use_bias)(d2)\n",
    "\n",
    "\n",
    "    # d3 = Dense(64,activation=activa,kernel_initializer=glorot_normal(),kernel_regularizer=l2(l2_penalty))(d2)\n",
    "    # d3 = Dense(256,activation=activa,kernel_initializer='random_uniform')(d2)\n",
    "    # d3 = Dense(256,activation=activa,kernel_initializer='random_uniform')(d3)\n",
    "\n",
    "\n",
    "    # d4 = Dense(low_dim,activation=activa,kernel_initializer=glorot_normal(),kernel_regularizer=l2(l2_penalty))(d3)\n",
    "    d4 = Dense(num_cells,activation='linear',kernel_initializer=kernel_init_func,kernel_regularizer=l2(l2_penalty),use_bias=use_bias)(d2)\n",
    "    ####################MLP################################################################################\n",
    "    centerloss_embed_layer = Embedding(num_cells, low_dim)(target_input)\n",
    "    centerloss_out = Lambda(lambda x: K.sum(K.square(x[0]-x[1][:,0]), 1, keepdims=True), name='center')([d2,centerloss_embed_layer])\n",
    "\n",
    "\n",
    "\n",
    "    softmax_out = Activation('softmax',name='softmax')(d4)\n",
    "\n",
    "    softmax_model = Model([SIMS_input,target_input],[softmax_out,centerloss_out])\n",
    "    softmax_model.compile(optimizer=adam(),loss=['categorical_crossentropy',lambda y_true,y_pred:y_pred],loss_weights=[1,0])\n",
    "    onehot_label = keras.utils.to_categorical(cell_idx-1,num_cells)\n",
    "    # reset_weights(softmax_model)\n",
    "\n",
    "\n",
    "    history=softmax_model.fit([train_x,cell_idx-1],[onehot_label,np.ones(shape=cell_idx.shape)],epochs=epochs,shuffle=True,batch_size=64)\n",
    "    while np.abs(history.history['loss'][-1]-np.max(history.history['loss']))<=error_threshold:\n",
    "        print('error')\n",
    "        reset_weights(softmax_model)\n",
    "#         history=softmax_model.fit([train_x,cell_idx-1,dummy_input_data],[onehot_label,np.ones(shape=cell_idx.shape)],epochs=epochs,shuffle=True,batch_size=64,verbose=verbose)\n",
    "        history=softmax_model.fit([train_x,cell_idx-1],[onehot_label,np.ones(shape=cell_idx.shape)],epochs=epochs,shuffle=True,batch_size=64)\n",
    "\n",
    "    logit_model = Model(SIMS_input,d4)\n",
    "\n",
    "    pred_logit = logit_model.predict(train_x)\n",
    "    rep_list = []\n",
    "    for t in t_list:\n",
    "        cur_representation = np.exp(pred_logit/t)\n",
    "        cur_representation = cur_representation/np.sum(cur_representation,axis=1,keepdims=True)\n",
    "        cur_representation = np.transpose(cur_representation)\n",
    "        rep_list.append(cur_representation)\n",
    "    return rep_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ID(a,t=5,epochs=200):\n",
    "    matter_list = np.array(a.var_names)\n",
    "    # SIMS_id_t_list = [5,10,15,20,25,30,35,40,50]\n",
    "    SIMS_id_t_list = [t]\n",
    "    train_x_tmp = a.uns['train_x']\n",
    "    cell_idx = a.uns['cell_idx']\n",
    "    num_cells = a.shape[0]\n",
    "\n",
    "    HEG_list = matter_list\n",
    "\n",
    "    HEG_col_idx = [list(matter_list).index(HEG) for HEG in HEG_list]\n",
    "\n",
    "    netwidths=[128,128,128]\n",
    "\n",
    "    error_threshold=0\n",
    "    train_x_HEG = train_x_tmp[:,HEG_col_idx]\n",
    "    train_x_preprocess = train_x_HEG\n",
    "\n",
    "\n",
    "    train_x_preprocess = (train_x_HEG)/np.sum(train_x_HEG,axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "    rep_list = get_distil_rep(train_x_preprocess,cell_idx,num_cells,SIMS_id_t_list, verbose=False,epochs=epochs,netwidths=netwidths,low_dim=netwidths[2],error_threshold=error_threshold)\n",
    "    a.obsm['ID'] = rep_list[0]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ID(a,epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.obsm['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SIMLR\n",
    "import scanpy as sc\n",
    "import time\n",
    "input_rep = a.obsm['ID']\n",
    "c=6\n",
    "simlr = SIMLR.SIMLR_LARGE(c, 10, 0); ###This is how we initialize an object for SIMLR. the first input is number of rank (clusters) and the second input is number of neighbors. The third one is an binary indicator whether to use memory-saving mode. you can turn it on when the number of cells are extremely large to save some memory but with the cost of efficiency.\n",
    "    \n",
    "S, F,val, ind = simlr.fit(input_rep)\n",
    "# print('Successfully Run SIMLR! SIMLR took %f seconds in total\\n' % (time.time() -         start_main))\n",
    "pred_y = simlr.fast_minibatch_kmeans(F,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SIMLR\n",
    "import scanpy as sc\n",
    "import time\n",
    "def run_SIMLR(a,c=8,rep='ID'):\n",
    "    if rep=='ID':\n",
    "        input_rep = a.obsm['ID']\n",
    "    elif rep=='Mean':\n",
    "        input_rep = a.X\n",
    "    \n",
    "    start_main = time.time()\n",
    "#     input_rep = SIMLR.helper.fast_pca(input_rep,100)\n",
    "    print(input_rep.shape,c)\n",
    "    simlr = SIMLR.SIMLR_LARGE(c, 10, 0) ###This is how we initialize an object for SIMLR. the first input is number of rank (clusters) and the second input is number of neighbors. The third one is an binary indicator whether to use memory-saving mode. you can turn it on when the number of cells are extremely large to save some memory but with the cost of efficiency.\n",
    "    \n",
    "    S, F,val, ind = simlr.fit(input_rep)\n",
    "    print('Successfully Run SIMLR! SIMLR took %f seconds in total\\n' % (time.time() -         start_main))\n",
    "    pred_y = simlr.fast_minibatch_kmeans(F,c)\n",
    "    print('done!')\n",
    "    a.obs['SIMLR'] =pred_y.astype('int').astype('str')\n",
    "    a.obs['SIMLR'] = a.obs['SIMLR'].astype('category')\n",
    "    \n",
    "    return a\n",
    "\n",
    "def Cluster(a,method,cluster_param,rep='ID'):\n",
    "    if method=='SIMLR':\n",
    "        return run_SIMLR(a,c=cluster_param,rep=rep)\n",
    "    elif method=='Louvain':\n",
    "        start_main = time.time()\n",
    "#         sc.pp.neighbors(a,use_rep=rep,metric='cosine',n_neighbors=15)\n",
    "        sc.tl.louvain(a,resolution=cluster_param)\n",
    "        print('Successfully Run Louvain! Louvain took %f seconds in total\\n' % (time.time() -         start_main))\n",
    "        \n",
    "    elif method=='Leiden':\n",
    "        start_main = time.time()\n",
    "#         sc.pp.neighbors(a,use_rep=rep,metric='cosine',n_neighbors=15)\n",
    "        sc.tl.leiden(a,resolution=cluster_param)\n",
    "        print('Successfully Run Leiden! Leiden took %f seconds in total\\n' % (time.time() -         start_main))\n",
    "        \n",
    "        \n",
    "    return a\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Umap(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(a,color='SIMLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Cluster(a,method='SIMLR',cluster_param=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m_sc(a_use,pval_thre,cls,method='pval_topk'):\n",
    "#     a_use = a_m_hepa\n",
    "#     pval_thre = 0.01\n",
    "    sc.tl.rank_genes_groups(a_use,n_genes=a_use.shape[1],groupby=cls)\n",
    "    \n",
    "    rec2mat_fun = lambda rec: np.vstack([np.array(list(m)) for m in rec])\n",
    "    name_mat = a_use.uns['rank_genes_groups']['names']\n",
    "    pval_mat = a_use.uns['rank_genes_groups']['pvals_adj']\n",
    "    score_mat = a_use.uns['rank_genes_groups']['scores']\n",
    "    \n",
    "    \n",
    "    name_mat = rec2mat_fun(name_mat)\n",
    "    pval_mat = rec2mat_fun(pval_mat)\n",
    "    score_mat = rec2mat_fun(score_mat)\n",
    "    n_cls = name_mat.shape[1]\n",
    "\n",
    "    rst_list = []\n",
    "    for i in range(n_cls):\n",
    "        if method=='pval_thre':\n",
    "            cur_idx = (pval_mat[:,i]<=pval_thre)\n",
    "        elif method=='pval_topk':\n",
    "            kth_p = np.sort(pval_mat[:,i])[pval_thre]\n",
    "            cur_idx = (pval_mat[:,i]<=kth_p)\n",
    "        elif method=='score_thre':\n",
    "            cur_idx = (score_mat[:,i]>=pval_thre)\n",
    "        elif method=='score_topk':\n",
    "            kth_score = np.flip(np.sort(score_mat[:,i]))[pval_thre]\n",
    "            cur_idx = (score_mat[:,i]>=kth_score)\n",
    "            \n",
    "        cur_m = name_mat[cur_idx,i]\n",
    "        rst_list.append(cur_m)\n",
    "        print(cur_m.shape[0])\n",
    "    a_use.uns[cls+'_mz'] = rst_list\n",
    "    return a_use\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = get_m_sc(a,10,cls='SIMLR',method='pval_topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import *\n",
    "\n",
    "def get_dist_mat_emd(a,method='emd'):\n",
    "\n",
    "    train_x = a.uns['train_x']\n",
    "    num_cells = a.shape[0]\n",
    "    num_features = a.shape[1]\n",
    "    cell_idx = a.uns['cell_idx']\n",
    "    cell_pixel_dict = {}\n",
    "    pixel_count = []\n",
    "    for i in range(num_cells):\n",
    "        cur_pixels = train_x[cell_idx==i+1,:]\n",
    "        cell_pixel_dict[i] = cur_pixels\n",
    "        pixel_count.append(cur_pixels.shape[0])\n",
    "\n",
    "    dist_mat = np.zeros(shape=(num_features,num_cells,num_cells))\n",
    "    for k in range(num_features):\n",
    "        if k%10==0:\n",
    "            print(k)\n",
    "#         print(k)\n",
    "        if method=='emd':\n",
    "            for i in range(num_cells):\n",
    "                for j in range(num_cells):\n",
    "                    cur_dist = wasserstein_distance(cell_pixel_dict[i][:,k],cell_pixel_dict[j][:,k])\n",
    "        #             cur_dist = euclidean(np.mean(cell_pixel_dict[i][:,k]),np.mean(cell_pixel_dict[j][:,k]))\n",
    "        #             print(k,i,j,cur_dist)\n",
    "#                     cur_dist = np.mean(cell_pixel_dict[i][:,k])-np.mean(cell_pixel_dict[j][:,k])\n",
    "                    dist_mat[k,i,j] = cur_dist\n",
    "        elif method=='euclidean':\n",
    "            cur_dist_mat = squareform(pdist(a.X[:,k][:,None]))\n",
    "            dist_mat[k,:,:] = cur_dist_mat\n",
    "    a.uns['feature_wise_distmat'] = dist_mat\n",
    "    return a\n",
    "\n",
    "def get_wbr(dist_mat,pred_list):\n",
    "    wbr_list = []\n",
    "    for i in range(dist_mat.shape[0]):\n",
    "        cur_dist_mat = dist_mat[i,:,:]\n",
    "        within_sum_1 = np.sum(dist_mat[i,:,:][pred_list==1,:][:,pred_list==1])\n",
    "        within_sum_0 = np.sum(dist_mat[i,:,:][pred_list==0,:][:,pred_list==0])    \n",
    "        between_sum_1 = np.sum(dist_mat[i,:,:][pred_list==1,:][:,pred_list==0])\n",
    "        between_sum_0 = np.sum(dist_mat[i,:,:][pred_list==0,:][:,pred_list==1])  \n",
    "        wbr = (within_sum_1+within_sum_0)/(between_sum_1+between_sum_0)\n",
    "        wbr_list.append(wbr)\n",
    "    return np.array(wbr_list)\n",
    "\n",
    "def get_wbr_mat(a_use,cls):\n",
    "#     a_use = a\n",
    "# #     wbr_thre = 10\n",
    "#     cls = 'SIMLR'\n",
    "#     method='topk'\n",
    "    dist_mat = a_use.uns['feature_wise_distmat']\n",
    "    unique_labels = np.unique(a_use.obs[cls])\n",
    "    wbr_mat = np.zeros(shape=(a_use.shape[1],unique_labels.shape[0]))\n",
    "\n",
    "    for i in range(wbr_mat.shape[1]):\n",
    "        cur_label = unique_labels[i]\n",
    "        cur_pred = a_use.obs[cls].copy().astype('str')\n",
    "        cur_pred[cur_pred!=cur_label] = -1\n",
    "        cur_pred[cur_pred==cur_label] = 0\n",
    "        cur_pred = -cur_pred\n",
    "        cur_wbr_list = get_wbr(dist_mat,cur_pred)\n",
    "        wbr_mat[:,i] = cur_wbr_list\n",
    "\n",
    "    a_use.uns['rank_genes_groups_emd'] = {\n",
    "        'scores':wbr_mat\n",
    "\n",
    "    }\n",
    "    return a_use\n",
    "\n",
    "\n",
    "def get_m_emd(a_use,thre,cls,method='topk',alg='emd'):\n",
    "    a_use = get_dist_mat_emd(a_use,method=alg)\n",
    "    a_use = get_wbr_mat(a_use,cls)\n",
    "    \n",
    "    score_mat = a_use.uns['rank_genes_groups_emd']['scores']\n",
    "    n_cls = score_mat.shape[1]\n",
    "    name_list = a_use.var_names\n",
    "    rst_list = []\n",
    "    for i in range(n_cls):\n",
    "        if method=='thre':\n",
    "            cur_idx = (score_mat[:,i]<=thre)\n",
    "        elif method=='topk':\n",
    "            kth_p = np.sort(score_mat[:,i])[thre]\n",
    "            cur_idx = (score_mat[:,i]<=kth_p)\n",
    "        \n",
    "            \n",
    "        cur_m = np.array(name_list[cur_idx])\n",
    "        rst_list.append(cur_m)\n",
    "        print(cur_m.shape[0])\n",
    "    a_use.uns[cls+'_mz_emd'] = rst_list\n",
    "    return a_use\n",
    "\n",
    "# def Diff(a_use,cls,method='sc'):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.uns['rank_genes_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_use = get_m_emd(a,20,'SIMLR',method='topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_use.uns['SIMLR_mz_emd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.tl.View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.preprocessing import *\n",
    "def View(a,method='Umap'):\n",
    "    \n",
    "    data_all = a.uns['IMS']\n",
    "    pseudo_count=1\n",
    "    data_all_norm = (data_all+pseudo_count)/(np.percentile(data_all,50,axis=1,keepdims=True)+pseudo_count)\n",
    "    data_all_norm = MinMaxScaler().fit_transform(data_all_norm)\n",
    "    if method=='Umap':\n",
    "        fg_umap = umap.UMAP(n_components=3,n_neighbors=50).fit_transform(data_all_norm)\n",
    "        a.uns['IMS_Umap'] = fg_umap\n",
    "    elif method=='Tsne':\n",
    "        fg_tsne = TSNE(n_components=3).fit_transform(data_all_norm)\n",
    "        a.uns['IMS_Tsne'] = fg_tsne\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = View(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_SIMS_view(a.uns['IMS_Umap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.IMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def get_mz_img(a,mz):\n",
    "    m_list = a.var_names\n",
    "    img_flatten = a.uns['IMS'][:,m_list==mz]\n",
    "    img_square = img_flatten.reshape(256,256)\n",
    "    return img_square\n",
    "\n",
    "def IMS(a,mz):\n",
    "    mz_img = get_mz_img(a,mz)\n",
    "    plt.imshow(mz_img)  \n",
    "    plt.title(mz+' m/z')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMS(a,'59.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut(a,method='dot'):\n",
    "    if method=='dot':\n",
    "        fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "        sc.pl.embedding(a,basis='spatial',ax=ax)\n",
    "    elif method=='mask':\n",
    "        pred_y = np.ones(shape=(a.shape[0],))\n",
    "        cmp = ['w']\n",
    "        plot_label_image(a,pred_y,cmp,save=None,mask=None,figsize=(5,5),anno=False,ifshow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cut(a,method='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cut(a,method='mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID(a):\n",
    "    sc.pl.umap(a,color='SIMLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster(a,cls,groups,method='mask'):\n",
    "#     groups are list of label index\n",
    "    if method=='mask':\n",
    "        plot_label_image(a,a.obs[cls],a.uns[cls+'_colors'],mask=groups,save=None)\n",
    "    elif method=='dot':\n",
    "        unique_labels = np.unique(a.obs[cls])\n",
    "        sc.pl.embedding(a,basis='spatial',color=cls,groups=list(unique_labels[groups]))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster(a,'SIMLR',[2,4],'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster(a,'SIMLR',[2,4],'dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_use.uns['SIMLR_mz_emd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(a,cls,method='SIMLR_mz_emd',show_gene_labels=False):\n",
    "    mzs = np.hstack(a.uns['SIMLR_mz_emd'])\n",
    "    cls='SIMLR'\n",
    "    sc.pl.heatmap(a,var_names=mzs,groupby=cls,standard_scale='var', \n",
    "                  cmap=heatmap_cmp,dendrogram=False,save=None,swap_axes=True,\n",
    "                 show_gene_labels=show_gene_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diff(a_use,'SIMLR',show_gene_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAM.pl.View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2lab, lab2rgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_SIMS_view(fg_umap,save=None):\n",
    "# fg_umap = a_concat[a_concat.obs['batch']=='0'].obsm['X_pca'][:,0:3]\n",
    "\n",
    "    fg_umap_norm = MinMaxScaler().fit_transform(fg_umap)\n",
    "    fg_umap_norm[:,0] = MinMaxScaler(feature_range=(0, 100)).fit_transform(fg_umap_norm[:,0][:,None])[:,0]\n",
    "    fg_umap_norm[:,1] = MinMaxScaler(feature_range=(-128, 127)).fit_transform(fg_umap_norm[:,1][:,None])[:,0]\n",
    "    fg_umap_norm[:,2] = MinMaxScaler(feature_range=(-128, 127)).fit_transform(fg_umap_norm[:,2][:,None])[:,0]\n",
    "#     fg_umap_norm[:,2] = MinMaxScaler(feature_range=(-128, 50)).fit_transform(fg_umap_norm[:,2][:,None])[:,0]\n",
    "\n",
    "\n",
    "    data_rgb = fg_umap_norm\n",
    "\n",
    "    data_rgb_img = data_rgb.reshape(256,256,3).astype('float64')\n",
    "    data_rgb_img = lab2rgb(data_rgb_img)\n",
    "    sns.set(style='white')\n",
    "    sns.set_color_codes('deep')\n",
    "    # sns.set\n",
    "    # cur_save_file = '{0}tsnemap_img_thres134_{1}_{2}.png'.format(save_path,str(bg_threshold),time_str)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(data_rgb_img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if save is not None:\n",
    "        plt.savefig(save,transparent=False,format='png',bbox_inches='tight')\n",
    "        \n",
    "def View(a,method='Umap'):\n",
    "    plot_SIMS_view(a.uns['IMS_'+method])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaden)",
   "language": "python",
   "name": "scaden"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
